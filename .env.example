# ======================================================
# PROJECT : Flight Risk Pipeline (SAMPLE ENV)
# ======================================================
# THIS IS A SAFE EXAMPLE CONFIG FOR PUBLIC REPOSITORIES.
# - Contains NO real API endpoints or secrets.
# - Replace values locally in config/settings.env (DO NOT commit).
# - For GitHub unban/safety: real ADS-B endpoints are intentionally omitted.
# ======================================================

# --------------------------
# Base / Project paths
# --------------------------
# Root folder where Spark, outputs and scripts exist
PROJECT_ROOT=/home/your_username/aircraft-risk-pipeline
# (optional alias used in some scripts)
BASE_DIR=${PROJECT_ROOT}

# Local python virtualenv (optional)
VENV_PY=${PROJECT_ROOT}/venv/bin/python3

# --------------------------
# Kafka configuration
# --------------------------
# Default local broker (safe)
KAFKA_BROKER=localhost:9092

# Topics
KAFKA_INPUT_TOPIC=aircraft_positions
KAFKA_OUTPUT_TOPIC=flight_risk_scores

# Optional auth placeholders (do NOT fill in public file)
KAFKA_USERNAME=
KAFKA_PASSWORD=

# --------------------------
# ADS-B data source (PLACEHOLDER)
# --------------------------
# IMPORTANT: These are placeholders only. Replace with real endpoints IN YOUR LOCAL FILE (config/settings.env)
# Example expected format (provider-specific):
#   https://<provider>/v2/point/{LAT}/{LON}/{RADIUS_KM}
#
# To avoid using live endpoints during demos or CI, set USE_MOCK_ADSB=true
USE_MOCK_ADSB=true

ZONE1=https://api.example.com/v2/point/LAT/LON/RADIUS_KM
ZONE2=https://api.example.com/v2/point/LAT/LON/RADIUS_KM
ZONE3=https://api.example.com/v2/point/LAT/LON/RADIUS_KM

# --------------------------
# Producer settings
# --------------------------
# Fetch interval in seconds (how often the producer polls the ADS-B endpoint)
FETCH_INTERVAL=5
HTTP_TIMEOUT=10
REQUEST_RETRIES=2

# --------------------------
# Spark configuration
# --------------------------
SPARK_MASTER=local[*]
SPARK_APP_NAME=AircraftFlightRiskPipeline
# If you use a packaged spark-submit, set path (optional)
# SPARK_SUBMIT=/path/to/spark/bin/spark-submit

# --------------------------
# Data paths (Bronze / Silver / Gold)
# --------------------------
DATA_PATH=${PROJECT_ROOT}/data

BRONZE_PATH=${DATA_PATH}/bronze/aircraft_positions
SILVER_PATH=${DATA_PATH}/silver/aircraft_cleaned
GOLD_PATH=${DATA_PATH}/gold

# --------------------------
# Streaming checkpoint dirs
# --------------------------
BRONZE_CHECKPOINT=${DATA_PATH}/checkpoints/bronze
SCORING_CHECKPOINT=${DATA_PATH}/checkpoints/scoring

# --------------------------
# Machine Learning paths
# --------------------------
FEATURES_PATH=${DATA_PATH}/ml/training_data/flight_features
MODEL_PATH=${DATA_PATH}/ml/models/flight_risk_model

# --------------------------
# Scoring / stream output (parquet)
# --------------------------
SCORING_PARQUET_OUT=${GOLD_PATH}/risk_scores/stream

# --------------------------
# Dashboard
# --------------------------
DASHBOARD_HOST=0.0.0.0
DASHBOARD_PORT=8501

# --------------------------
# Airflow (optional)
# --------------------------
AIRFLOW_HOME=${PROJECT_ROOT}/airflow
AIRFLOW_DAG_SCHEDULE=@daily

# --------------------------
# Misc / debug
# --------------------------
LOG_DIR=${PROJECT_ROOT}/logs
RUN_DIR=${PROJECT_ROOT}/run


